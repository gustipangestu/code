digraph {
	graph [size="46.949999999999996,46.949999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2453645817824 [label="
 ()" fillcolor=darkolivegreen1]
	2450867118640 [label=MeanBackward0]
	2450867118832 -> 2450867118640
	2450867118832 [label=AddmmBackward0]
	2450867117344 -> 2450867118832
	2449154561968 [label="fc2.bias
 (36)" fillcolor=lightblue]
	2449154561968 -> 2450867117344
	2450867117344 [label=AccumulateGrad]
	2450867119168 -> 2450867118832
	2450867119168 [label=ReluBackward0]
	2450867117104 -> 2450867119168
	2450867117104 [label=AddmmBackward0]
	2450867117920 -> 2450867117104
	2450435408496 [label="fc1.bias
 (100)" fillcolor=lightblue]
	2450435408496 -> 2450867117920
	2450867117920 [label=AccumulateGrad]
	2450867118400 -> 2450867117104
	2450867118400 [label=CatBackward0]
	2450867117008 -> 2450867118400
	2450867117008 [label=AddmmBackward0]
	2450867118352 -> 2450867117008
	2450548942752 [label="cnn.fc.bias
 (300)" fillcolor=lightblue]
	2450548942752 -> 2450867118352
	2450867118352 [label=AccumulateGrad]
	2450867118928 -> 2450867117008
	2450867118928 [label=ReshapeAliasBackward0]
	2450867117056 -> 2450867118928
	2450867117056 [label=MeanBackward1]
	2450867118112 -> 2450867117056
	2450867118112 [label=ReluBackward0]
	2450867022304 -> 2450867118112
	2450867022304 [label=AddBackward0]
	2450867022256 -> 2450867022304
	2450867022256 [label=NativeBatchNormBackward0]
	2450867021248 -> 2450867022256
	2450867021248 [label=ConvolutionBackward0]
	2450867011888 -> 2450867021248
	2450867011888 [label=ReluBackward0]
	2450867020096 -> 2450867011888
	2450867020096 [label=NativeBatchNormBackward0]
	2450867019808 -> 2450867020096
	2450867019808 [label=ConvolutionBackward0]
	2450867025472 -> 2450867019808
	2450867025472 [label=ReluBackward0]
	2450867027440 -> 2450867025472
	2450867027440 [label=AddBackward0]
	2450867022496 -> 2450867027440
	2450867022496 [label=NativeBatchNormBackward0]
	2450867024080 -> 2450867022496
	2450867024080 [label=ConvolutionBackward0]
	2450867024752 -> 2450867024080
	2450867024752 [label=ReluBackward0]
	2450867015440 -> 2450867024752
	2450867015440 [label=NativeBatchNormBackward0]
	2450867027680 -> 2450867015440
	2450867027680 [label=ConvolutionBackward0]
	2450867019760 -> 2450867027680
	2450867019760 [label=ReluBackward0]
	2450867012272 -> 2450867019760
	2450867012272 [label=AddBackward0]
	2450867020192 -> 2450867012272
	2450867020192 [label=NativeBatchNormBackward0]
	2450867020288 -> 2450867020192
	2450867020288 [label=ConvolutionBackward0]
	2450867020144 -> 2450867020288
	2450867020144 [label=ReluBackward0]
	2450867023168 -> 2450867020144
	2450867023168 [label=NativeBatchNormBackward0]
	2450867026624 -> 2450867023168
	2450867026624 [label=ConvolutionBackward0]
	2450867019616 -> 2450867026624
	2450867019616 [label=ReluBackward0]
	2450867022640 -> 2450867019616
	2450867022640 [label=AddBackward0]
	2450867022880 -> 2450867022640
	2450867022880 [label=NativeBatchNormBackward0]
	2450867022928 -> 2450867022880
	2450867022928 [label=ConvolutionBackward0]
	2450867023024 -> 2450867022928
	2450867023024 [label=ReluBackward0]
	2450867015152 -> 2450867023024
	2450867015152 [label=NativeBatchNormBackward0]
	2450867020336 -> 2450867015152
	2450867020336 [label=ConvolutionBackward0]
	2450867027344 -> 2450867020336
	2450867027344 [label=MaxPool2DWithIndicesBackward0]
	2450788034224 -> 2450867027344
	2450788034224 [label=ReluBackward0]
	2450788039888 -> 2450788034224
	2450788039888 [label=NativeBatchNormBackward0]
	2450788037632 -> 2450788039888
	2450788037632 [label=ConvolutionBackward0]
	2450788039984 -> 2450788037632
	2450788039984 [label=ReluBackward0]
	2450788037152 -> 2450788039984
	2450788037152 [label=NativeBatchNormBackward0]
	2450788034176 -> 2450788037152
	2450788034176 [label=ConvolutionBackward0]
	2450788035616 -> 2450788034176
	2450788035616 [label=ReluBackward0]
	2453645918896 -> 2450788035616
	2453645918896 [label=NativeBatchNormBackward0]
	2453645918656 -> 2453645918896
	2453645918656 [label=ConvolutionBackward0]
	2453645919232 -> 2453645918656
	2449154570208 [label="cnn.conv1.0.weight
 (24, 3, 3, 3)" fillcolor=lightblue]
	2449154570208 -> 2453645919232
	2453645919232 [label=AccumulateGrad]
	2453645918992 -> 2453645918896
	2450428121296 [label="cnn.conv1.1.weight
 (24)" fillcolor=lightblue]
	2450428121296 -> 2453645918992
	2453645918992 [label=AccumulateGrad]
	2453645918416 -> 2453645918896
	2450426814736 [label="cnn.conv1.1.bias
 (24)" fillcolor=lightblue]
	2450426814736 -> 2453645918416
	2453645918416 [label=AccumulateGrad]
	2450788040464 -> 2450788034176
	2449339783728 [label="cnn.conv1.3.weight
 (32, 24, 3, 3)" fillcolor=lightblue]
	2449339783728 -> 2450788040464
	2450788040464 [label=AccumulateGrad]
	2450788035520 -> 2450788037152
	2449154570128 [label="cnn.conv1.4.weight
 (32)" fillcolor=lightblue]
	2449154570128 -> 2450788035520
	2450788035520 [label=AccumulateGrad]
	2450788034800 -> 2450788037152
	2449154570048 [label="cnn.conv1.4.bias
 (32)" fillcolor=lightblue]
	2449154570048 -> 2450788034800
	2450788034800 [label=AccumulateGrad]
	2450788033792 -> 2450788037632
	2449154569568 [label="cnn.conv1.6.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	2449154569568 -> 2450788033792
	2450788033792 [label=AccumulateGrad]
	2450788030912 -> 2450788039888
	2449154569648 [label="cnn.bn1.weight
 (64)" fillcolor=lightblue]
	2449154569648 -> 2450788030912
	2450788030912 [label=AccumulateGrad]
	2450788040224 -> 2450788039888
	2449154569488 [label="cnn.bn1.bias
 (64)" fillcolor=lightblue]
	2449154569488 -> 2450788040224
	2450788040224 [label=AccumulateGrad]
	2450788027840 -> 2450867020336
	2449154569008 [label="cnn.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2449154569008 -> 2450788027840
	2450788027840 [label=AccumulateGrad]
	2450867026432 -> 2450867015152
	2449154569088 [label="cnn.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2449154569088 -> 2450867026432
	2450867026432 [label=AccumulateGrad]
	2450867023216 -> 2450867015152
	2449154568928 [label="cnn.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2449154568928 -> 2450867023216
	2450867023216 [label=AccumulateGrad]
	2450867027056 -> 2450867022928
	2449154568528 [label="cnn.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2449154568528 -> 2450867027056
	2450867027056 [label=AccumulateGrad]
	2450867011744 -> 2450867022880
	2449154568448 [label="cnn.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2449154568448 -> 2450867011744
	2450867011744 [label=AccumulateGrad]
	2450867022400 -> 2450867022880
	2449154568368 [label="cnn.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2449154568368 -> 2450867022400
	2450867022400 [label=AccumulateGrad]
	2450867027344 -> 2450867022640
	2450867023648 -> 2450867026624
	2449154567088 [label="cnn.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2449154567088 -> 2450867023648
	2450867023648 [label=AccumulateGrad]
	2450867020240 -> 2450867023168
	2449154567168 [label="cnn.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2449154567168 -> 2450867020240
	2450867020240 [label=AccumulateGrad]
	2450867019664 -> 2450867023168
	2449154567008 [label="cnn.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2449154567008 -> 2450867019664
	2450867019664 [label=AccumulateGrad]
	2450867020528 -> 2450867020288
	2449154566528 [label="cnn.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2449154566528 -> 2450867020528
	2450867020528 [label=AccumulateGrad]
	2450867024656 -> 2450867020192
	2449154566448 [label="cnn.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2449154566448 -> 2450867024656
	2450867024656 [label=AccumulateGrad]
	2450867024560 -> 2450867020192
	2449154566368 [label="cnn.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2449154566368 -> 2450867024560
	2450867024560 [label=AccumulateGrad]
	2450867019088 -> 2450867012272
	2450867019088 [label=NativeBatchNormBackward0]
	2450867019424 -> 2450867019088
	2450867019424 [label=ConvolutionBackward0]
	2450867024032 -> 2450867019424
	2450867024032 [label=AvgPool2DBackward0]
	2450867019616 -> 2450867024032
	2450867022352 -> 2450867019424
	2449154567968 [label="cnn.layer2.0.downsample.1.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2449154567968 -> 2450867022352
	2450867022352 [label=AccumulateGrad]
	2450867020720 -> 2450867019088
	2449154567888 [label="cnn.layer2.0.downsample.2.weight
 (128)" fillcolor=lightblue]
	2449154567888 -> 2450867020720
	2450867020720 [label=AccumulateGrad]
	2450867024368 -> 2450867019088
	2449154567808 [label="cnn.layer2.0.downsample.2.bias
 (128)" fillcolor=lightblue]
	2449154567808 -> 2450867024368
	2450867024368 [label=AccumulateGrad]
	2450867015680 -> 2450867027680
	2449154565168 [label="cnn.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2449154565168 -> 2450867015680
	2450867015680 [label=AccumulateGrad]
	2450867015776 -> 2450867015440
	2449154565248 [label="cnn.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2449154565248 -> 2450867015776
	2450867015776 [label=AccumulateGrad]
	2450867024272 -> 2450867015440
	2449154565088 [label="cnn.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2449154565088 -> 2450867024272
	2450867024272 [label=AccumulateGrad]
	2450867024320 -> 2450867024080
	2449154564608 [label="cnn.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2449154564608 -> 2450867024320
	2450867024320 [label=AccumulateGrad]
	2450867015584 -> 2450867022496
	2449154567408 [label="cnn.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2449154567408 -> 2450867015584
	2450867015584 [label=AccumulateGrad]
	2450867024176 -> 2450867022496
	2449154567328 [label="cnn.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2449154567328 -> 2450867024176
	2450867024176 [label=AccumulateGrad]
	2450867021632 -> 2450867027440
	2450867021632 [label=NativeBatchNormBackward0]
	2450867024224 -> 2450867021632
	2450867024224 [label=ConvolutionBackward0]
	2450867024896 -> 2450867024224
	2450867024896 [label=AvgPool2DBackward0]
	2450867019760 -> 2450867024896
	2450867023408 -> 2450867024224
	2449154565888 [label="cnn.layer3.0.downsample.1.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2449154565888 -> 2450867023408
	2450867023408 [label=AccumulateGrad]
	2450867012704 -> 2450867021632
	2449154565808 [label="cnn.layer3.0.downsample.2.weight
 (256)" fillcolor=lightblue]
	2449154565808 -> 2450867012704
	2450867012704 [label=AccumulateGrad]
	2450867027392 -> 2450867021632
	2449154565728 [label="cnn.layer3.0.downsample.2.bias
 (256)" fillcolor=lightblue]
	2449154565728 -> 2450867027392
	2450867027392 [label=AccumulateGrad]
	2450867018944 -> 2450867019808
	2449154563408 [label="cnn.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2449154563408 -> 2450867018944
	2450867018944 [label=AccumulateGrad]
	2450867019904 -> 2450867020096
	2449154563488 [label="cnn.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2449154563488 -> 2450867019904
	2450867019904 [label=AccumulateGrad]
	2450867026912 -> 2450867020096
	2449154563328 [label="cnn.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2449154563328 -> 2450867026912
	2450867026912 [label=AccumulateGrad]
	2450867023360 -> 2450867021248
	2449154562848 [label="cnn.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2449154562848 -> 2450867023360
	2450867023360 [label=AccumulateGrad]
	2450867023264 -> 2450867022256
	2449154562528 [label="cnn.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2449154562528 -> 2450867023264
	2450867023264 [label=AccumulateGrad]
	2450867022592 -> 2450867022256
	2449154562768 [label="cnn.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2449154562768 -> 2450867022592
	2450867022592 [label=AccumulateGrad]
	2450867021584 -> 2450867022304
	2450867021584 [label=NativeBatchNormBackward0]
	2450867024992 -> 2450867021584
	2450867024992 [label=ConvolutionBackward0]
	2450867015632 -> 2450867024992
	2450867015632 [label=AvgPool2DBackward0]
	2450867025472 -> 2450867015632
	2450867025040 -> 2450867024992
	2449154564128 [label="cnn.layer4.0.downsample.1.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2449154564128 -> 2450867025040
	2450867025040 [label=AccumulateGrad]
	2450867025568 -> 2450867021584
	2449154564048 [label="cnn.layer4.0.downsample.2.weight
 (512)" fillcolor=lightblue]
	2449154564048 -> 2450867025568
	2450867025568 [label=AccumulateGrad]
	2450867019856 -> 2450867021584
	2449154563968 [label="cnn.layer4.0.downsample.2.bias
 (512)" fillcolor=lightblue]
	2449154563968 -> 2450867019856
	2450867019856 [label=AccumulateGrad]
	2450867117776 -> 2450867117008
	2450867117776 [label=TBackward0]
	2450867118160 -> 2450867117776
	2450473424336 [label="cnn.fc.weight
 (300, 512)" fillcolor=lightblue]
	2450473424336 -> 2450867118160
	2450867118160 [label=AccumulateGrad]
	2450867111056 -> 2450867117104
	2450867111056 [label=TBackward0]
	2450867118304 -> 2450867111056
	2450436630192 [label="fc1.weight
 (100, 363)" fillcolor=lightblue]
	2450436630192 -> 2450867118304
	2450867118304 [label=AccumulateGrad]
	2450867118208 -> 2450867118832
	2450867118208 [label=TBackward0]
	2450867116960 -> 2450867118208
	2449154562288 [label="fc2.weight
 (36, 100)" fillcolor=lightblue]
	2449154562288 -> 2450867116960
	2450867116960 [label=AccumulateGrad]
	2450867118640 -> 2453645817824
}
