{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class       above\n",
       "namaFile    00430\n",
       "urutan          0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lengkap = pd.read_pickle(\"data_keypoint_fitur_lengkap_pickle\")\n",
    "# data_lengkap = pd.read_pickle(\"list_file\")\n",
    "# data_lengkap.iloc[0][\"triangle_feature\"]\n",
    "a = data_lengkap\n",
    "a.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class      yourself\n",
       "file          64445\n",
       "feature          []\n",
       "Name: 643, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new database triangle\n",
    "import os\n",
    "\n",
    "def buatDatasetLLM(dataset):\n",
    "    root_dir=\"../../dataset/extractFrame/\"\n",
    "    classAwal = dataset[\"class\"][0]\n",
    "    fileAwal = dataset[\"namaFile\"][0]\n",
    "    index = dataset['urutan'][0]\n",
    "    stack = []\n",
    "    datasetLLM = []\n",
    "    for i in range(0,len(dataset)):\n",
    "        classSekarang = dataset[\"class\"][i]\n",
    "        fileSekarang = dataset['namaFile'][i]\n",
    "        indexSekarang = str(dataset['urutan'][i])\n",
    "        file_path = os.path.join(root_dir, classSekarang, fileSekarang, indexSekarang)\n",
    "        file_path = file_path.replace(os.sep, '/')\n",
    "        if(classAwal == classSekarang):\n",
    "            if(fileAwal == fileSekarang):\n",
    "                # stack.append(dataset['namaFile'][i])\n",
    "                stack.append(file_path)\n",
    "            else:\n",
    "                kelas = {\n",
    "                'class': classSekarang,\n",
    "                'file': fileSekarang,\n",
    "                'feature': stack,\n",
    "                }\n",
    "                datasetLLM.append(kelas)\n",
    "                fileAwal = fileSekarang\n",
    "                stack = []\n",
    "        else:\n",
    "            kelas = {\n",
    "                'class': classSekarang,\n",
    "                'file': fileSekarang,\n",
    "                'feature': stack,\n",
    "                }\n",
    "            datasetLLM.append(kelas)\n",
    "            classAwal = classSekarang\n",
    "            stack = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    return datasetLLM\n",
    "        \n",
    "            \n",
    "\n",
    "dts_triangle = buatDatasetLLM(data_lengkap)\n",
    "dts_triangle = pd.DataFrame(dts_triangle)\n",
    "dts_triangle.iloc[643]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../dataset/extractFrame/above/00430/0.jpg',\n",
       " '../../dataset/extractFrame/above/00430/1.jpg',\n",
       " '../../dataset/extractFrame/above/00430/2.jpg',\n",
       " '../../dataset/extractFrame/above/00430/3.jpg',\n",
       " '../../dataset/extractFrame/above/00430/4.jpg',\n",
       " '../../dataset/extractFrame/above/00430/5.jpg',\n",
       " '../../dataset/extractFrame/above/00430/6.jpg',\n",
       " '../../dataset/extractFrame/above/00430/7.jpg',\n",
       " '../../dataset/extractFrame/above/00430/8.jpg',\n",
       " '../../dataset/extractFrame/above/00430/9.jpg',\n",
       " '../../dataset/extractFrame/above/00430/10.jpg',\n",
       " '../../dataset/extractFrame/above/00430/11.jpg',\n",
       " '../../dataset/extractFrame/above/00430/12.jpg',\n",
       " '../../dataset/extractFrame/above/00430/13.jpg',\n",
       " '../../dataset/extractFrame/above/00430/14.jpg',\n",
       " '../../dataset/extractFrame/above/00430/15.jpg',\n",
       " '../../dataset/extractFrame/above/00430/16.jpg',\n",
       " '../../dataset/extractFrame/above/00430/17.jpg',\n",
       " '../../dataset/extractFrame/above/00430/18.jpg',\n",
       " '../../dataset/extractFrame/above/00430/19.jpg',\n",
       " '../../dataset/extractFrame/above/00430/20.jpg',\n",
       " '../../dataset/extractFrame/above/00430/21.jpg',\n",
       " '../../dataset/extractFrame/above/00430/22.jpg',\n",
       " '../../dataset/extractFrame/above/00430/23.jpg',\n",
       " '../../dataset/extractFrame/above/00430/24.jpg',\n",
       " '../../dataset/extractFrame/above/00430/25.jpg',\n",
       " '../../dataset/extractFrame/above/00430/26.jpg',\n",
       " '../../dataset/extractFrame/above/00430/27.jpg',\n",
       " '../../dataset/extractFrame/above/00430/28.jpg',\n",
       " '../../dataset/extractFrame/above/00430/29.jpg',\n",
       " '../../dataset/extractFrame/above/00430/30.jpg',\n",
       " '../../dataset/extractFrame/above/00430/31.jpg',\n",
       " '../../dataset/extractFrame/above/00430/32.jpg',\n",
       " '../../dataset/extractFrame/above/00430/33.jpg',\n",
       " '../../dataset/extractFrame/above/00430/34.jpg',\n",
       " '../../dataset/extractFrame/above/00430/35.jpg',\n",
       " '../../dataset/extractFrame/above/00430/36.jpg',\n",
       " '../../dataset/extractFrame/above/00430/37.jpg',\n",
       " '../../dataset/extractFrame/above/00430/38.jpg',\n",
       " '../../dataset/extractFrame/above/00430/39.jpg',\n",
       " '../../dataset/extractFrame/above/00430/40.jpg',\n",
       " '../../dataset/extractFrame/above/00430/41.jpg',\n",
       " '../../dataset/extractFrame/above/00430/42.jpg',\n",
       " '../../dataset/extractFrame/above/00430/43.jpg',\n",
       " '../../dataset/extractFrame/above/00430/44.jpg',\n",
       " '../../dataset/extractFrame/above/00430/45.jpg',\n",
       " '../../dataset/extractFrame/above/00430/46.jpg',\n",
       " '../../dataset/extractFrame/above/00430/47.jpg',\n",
       " '../../dataset/extractFrame/above/00430/48.jpg',\n",
       " '../../dataset/extractFrame/above/00430/49.jpg',\n",
       " '../../dataset/extractFrame/above/00430/50.jpg',\n",
       " '../../dataset/extractFrame/above/00430/51.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bikin_data_fix(directory):\n",
    "    _kumpulanFrame = []\n",
    "    _datasetnya = []\n",
    "    for kelas in os.listdir(directory):\n",
    "        # print(kelas)\n",
    "        for file in os.listdir(os.path.join(directory, kelas)):\n",
    "            # print(file)\n",
    "            _nomorFrame = 0\n",
    "            for frameExt in os.listdir(os.path.join(directory, kelas, file)):\n",
    "                _extension = os.path.splitext(frameExt)\n",
    "                _namaFile = f\"{str(_nomorFrame)}{_extension[1]}\"\n",
    "                _pathLengkap = os.path.join(directory, kelas, file, _namaFile)\n",
    "                _pathLengkap = _pathLengkap.replace(os.sep, '/')\n",
    "                # print(_extension[1])\n",
    "                # print(_namaFile)\n",
    "                # print(_pathLengkap)\n",
    "                _kumpulanFrame.append(_pathLengkap)\n",
    "                _nomorFrame = _nomorFrame + 1\n",
    "            jelas = {\n",
    "                    'class': kelas,\n",
    "                    'folder': file,\n",
    "                    'file': _kumpulanFrame,\n",
    "                }\n",
    "            _datasetnya.append(jelas)\n",
    "            _kumpulanFrame = []\n",
    "    return _datasetnya\n",
    "            \n",
    "root_dir=\"../../dataset/extractFrame/\"\n",
    "aaa = bikin_data_fix(root_dir)\n",
    "dfaaa = pd.DataFrame(aaa)\n",
    "dfaaa.to_pickle('file_list_lengkap_fix')\n",
    "dfaaa.iloc[0]['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../dataset/extractFrame/above/00430/0.jpg',\n",
       " '../../dataset/extractFrame/above/00430/1.jpg',\n",
       " '../../dataset/extractFrame/above/00430/2.jpg',\n",
       " '../../dataset/extractFrame/above/00430/3.jpg',\n",
       " '../../dataset/extractFrame/above/00430/4.jpg',\n",
       " '../../dataset/extractFrame/above/00430/5.jpg',\n",
       " '../../dataset/extractFrame/above/00430/6.jpg',\n",
       " '../../dataset/extractFrame/above/00430/7.jpg',\n",
       " '../../dataset/extractFrame/above/00430/8.jpg',\n",
       " '../../dataset/extractFrame/above/00430/9.jpg',\n",
       " '../../dataset/extractFrame/above/00430/10.jpg',\n",
       " '../../dataset/extractFrame/above/00430/11.jpg',\n",
       " '../../dataset/extractFrame/above/00430/12.jpg',\n",
       " '../../dataset/extractFrame/above/00430/13.jpg',\n",
       " '../../dataset/extractFrame/above/00430/14.jpg',\n",
       " '../../dataset/extractFrame/above/00430/15.jpg',\n",
       " '../../dataset/extractFrame/above/00430/16.jpg',\n",
       " '../../dataset/extractFrame/above/00430/17.jpg',\n",
       " '../../dataset/extractFrame/above/00430/18.jpg',\n",
       " '../../dataset/extractFrame/above/00430/19.jpg',\n",
       " '../../dataset/extractFrame/above/00430/20.jpg',\n",
       " '../../dataset/extractFrame/above/00430/21.jpg',\n",
       " '../../dataset/extractFrame/above/00430/22.jpg',\n",
       " '../../dataset/extractFrame/above/00430/23.jpg',\n",
       " '../../dataset/extractFrame/above/00430/24.jpg',\n",
       " '../../dataset/extractFrame/above/00430/25.jpg',\n",
       " '../../dataset/extractFrame/above/00430/26.jpg',\n",
       " '../../dataset/extractFrame/above/00430/27.jpg',\n",
       " '../../dataset/extractFrame/above/00430/28.jpg',\n",
       " '../../dataset/extractFrame/above/00430/29.jpg',\n",
       " '../../dataset/extractFrame/above/00430/30.jpg',\n",
       " '../../dataset/extractFrame/above/00430/31.jpg',\n",
       " '../../dataset/extractFrame/above/00430/32.jpg',\n",
       " '../../dataset/extractFrame/above/00430/33.jpg',\n",
       " '../../dataset/extractFrame/above/00430/34.jpg',\n",
       " '../../dataset/extractFrame/above/00430/35.jpg',\n",
       " '../../dataset/extractFrame/above/00430/36.jpg',\n",
       " '../../dataset/extractFrame/above/00430/37.jpg',\n",
       " '../../dataset/extractFrame/above/00430/38.jpg',\n",
       " '../../dataset/extractFrame/above/00430/39.jpg',\n",
       " '../../dataset/extractFrame/above/00430/40.jpg',\n",
       " '../../dataset/extractFrame/above/00430/41.jpg',\n",
       " '../../dataset/extractFrame/above/00430/42.jpg',\n",
       " '../../dataset/extractFrame/above/00430/43.jpg',\n",
       " '../../dataset/extractFrame/above/00430/44.jpg',\n",
       " '../../dataset/extractFrame/above/00430/45.jpg',\n",
       " '../../dataset/extractFrame/above/00430/46.jpg',\n",
       " '../../dataset/extractFrame/above/00430/47.jpg',\n",
       " '../../dataset/extractFrame/above/00430/48.jpg',\n",
       " '../../dataset/extractFrame/above/00430/49.jpg',\n",
       " '../../dataset/extractFrame/above/00430/50.jpg',\n",
       " '../../dataset/extractFrame/above/00430/51.jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dts_triangle.to_pickle('file_list_lengkap_fix')\n",
    "data = pd.read_pickle('file_list_lengkap_fix')\n",
    "data['file'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\miniconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_32_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_32_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_proj: torch.Size([1, 768, 7, 7])\n",
      "encoder: torch.Size([1, 50, 768])\n",
      "heads: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vit_b_32, resnet101\n",
    "\n",
    "class VerboseExecution(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"{layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "verbose_resnet = VerboseExecution(vit_b_32(pretrained=True))\n",
    "dummy_input = torch.ones(1, 3, 224, 224)\n",
    "\n",
    "_ = verbose_resnet(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 1, 1])\n",
      "tensor([[[[0.4772]],\n",
      "\n",
      "         [[0.4789]],\n",
      "\n",
      "         [[0.7124]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.4702]],\n",
      "\n",
      "         [[0.4955]],\n",
      "\n",
      "         [[0.3755]]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Iterable, Callable\n",
    "import numpy as np\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model: nn.Module, layers: Iterable[str]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "\n",
    "        for layer_id in layers:\n",
    "            layer = dict([*self.model.named_modules()])[layer_id]\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_id))\n",
    "\n",
    "    def save_outputs_hook(self, layer_id: str) -> Callable:\n",
    "        def fn(_, __, output):\n",
    "            self._features[layer_id] = output\n",
    "        return fn\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        _ = self.model(x)\n",
    "        return self._features\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "img = cv2.imread('29.jpg')\n",
    "img = cv2.resize(img, (100,100))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "trasnform = transform.ToTensor()\n",
    "img = trasnform(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img_tensor = torch.tensor(img)\n",
    "print(type(img))\n",
    "print(type(img_tensor))\n",
    "print(type(dummy_input))\n",
    "fiturCNN = FeatureExtractor(resnet101(pretrained=True), layers=[\"avgpool\"])\n",
    "features = fiturCNN(img_tensor)\n",
    "print(features['avgpool'].shape)\n",
    "print(features['avgpool'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFitur(_dataset):\n",
    "    stack = []\n",
    "    \n",
    "    itung = 0\n",
    "    for i in range(0, len(_dataset)-1):\n",
    "        print(_dataset[i])\n",
    "        # _path = os.path(_dataset[i])\n",
    "        _gambar = cv2.imread(_dataset[i])\n",
    "        _r = 100.0 / _gambar.shape[0]\n",
    "        _dim = (int(_gambar.shape[1] * _r), 100)\n",
    "        _gambarResize = cv2.resize(_gambar, (_dim))\n",
    "        stack.append(_gambarResize)\n",
    "    return stack\n",
    "\n",
    "# data['image'] = data['file'].apply(extractFitur)\n",
    "# data\n",
    "# print(type(data['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# data.to_pickle(\"../../dataset/dataframe_image_lengkap\")\n",
    "\n",
    "# plt.imshow(data['image'].iloc[0][0])\n",
    "\n",
    "# load data\n",
    "dataframeImage = pd.read_pickle('../../dataset/dataframe_image_lengkap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 47.0ms\n",
      "Speed: 3.0ms preprocess, 47.0ms inference, 55.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 0.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 0.5ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 0.5ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 0.5ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 0.5ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 0.5ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.5ms\n",
      "Speed: 0.5ms preprocess, 8.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.5ms\n",
      "Speed: 1.0ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 0.5ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 0.5ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 0.5ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 0.5ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 0.5ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 0.5ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 0.5ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 0.5ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 1.0ms preprocess, 15.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _res\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# dataframeImage['resnet101Fitur'] = dataframeImage['image'].apply(extractWithPretrainedModel)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# wuhuy = dataframeImage.iloc[:2]['image'].apply(extractWithPretrainedModel)\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m dataframeImage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLO11\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataframeImage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extractPoseYOLO)\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m, in \u001b[0;36mextractPoseYOLO\u001b[1;34m(_data)\u001b[0m\n\u001b[0;32m     32\u001b[0m _em \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m168\u001b[39m,\u001b[38;5;241m51\u001b[39m) \u001b[38;5;66;03m#karena jumlah fiturnya 51\u001b[39;00m\n\u001b[0;32m     33\u001b[0m _stackTensor \u001b[38;5;241m=\u001b[39m _stack\n\u001b[1;32m---> 34\u001b[0m _padSeq \u001b[38;5;241m=\u001b[39m pad_sequence([_em, _stackTensor], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m _padSeq \u001b[38;5;241m=\u001b[39m _padSeq\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     36\u001b[0m _res \u001b[38;5;241m=\u001b[39m _padSeq[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:478\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[0;32m    474\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mpad_sequence(\n\u001b[0;32m    479\u001b[0m     sequences, batch_first, padding_value, padding_side  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    480\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "modelBest = YOLO('runs/pose/train/weights/best.pt')\n",
    "\n",
    "def extractWithPretrainedModel(_data):\n",
    "    _stack = []\n",
    "    for i in range(0, len(_data)-1):\n",
    "        _img = cv2.cvtColor(_data[i], cv2.COLOR_BGR2RGB)\n",
    "        _trasnform = transform.ToTensor()\n",
    "        _img = _trasnform(_img)\n",
    "        _img = np.expand_dims(_img, axis=0)\n",
    "        _img_tensor = torch.tensor(img)\n",
    "        _fiturCNN = FeatureExtractor(resnet101(pretrained=True), layers=[\"avgpool\"])\n",
    "        _features = _fiturCNN(_img_tensor)\n",
    "        _stack.append(_features['avgpool'])\n",
    "    \n",
    "    return _stack\n",
    "\n",
    "def extractPoseYOLO(_data):\n",
    "    _stack = []\n",
    "    for i in range(0, len(_data)-1):\n",
    "        _img = cv2.cvtColor(_data[i], cv2.COLOR_BGR2RGB)\n",
    "        _hasil = modelBest(_img)\n",
    "        for _h in _hasil:\n",
    "            _keypoint = _h.keypoints.data\n",
    "            _flat = torch.flatten(_keypoint)\n",
    "            # _flat = _flat.cpu()\n",
    "            # _flatNumpy = _flat.detach().numpy()\n",
    "            \n",
    "        _stack.append(_flat)\n",
    "    _em = torch.rand(168,51) #karena jumlah fiturnya 51\n",
    "    _stackTensor = _stack\n",
    "    _padSeq = pad_sequence([_em, _stackTensor], batch_first=True, padding_value=0)\n",
    "    # _padSeq = _padSeq.cpu()\n",
    "    _res = _padSeq[1].numpy()\n",
    "        \n",
    "    return _res\n",
    "    \n",
    "    \n",
    "\n",
    "# dataframeImage['resnet101Fitur'] = dataframeImage['image'].apply(extractWithPretrainedModel)\n",
    "# wuhuy = dataframeImage.iloc[:2]['image'].apply(extractWithPretrainedModel)\n",
    "\n",
    "dataframeImage['YOLO11'] = dataframeImage['image'].apply(extractPoseYOLO)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>folder</th>\n",
       "      <th>file</th>\n",
       "      <th>image</th>\n",
       "      <th>YOLO11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>above</td>\n",
       "      <td>00430</td>\n",
       "      <td>[../../dataset/extractFrame/above/00430/0.jpg,...</td>\n",
       "      <td>[[[[6 6 6], [6 6 6], [6 6 6], [8 8 8], [6 6 6]...</td>\n",
       "      <td>[[tensor(88.6293), tensor(19.6030), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>above</td>\n",
       "      <td>00431</td>\n",
       "      <td>[../../dataset/extractFrame/above/00431/0.jpg,...</td>\n",
       "      <td>[[[[128 161  99], [126 159  98], [124 157  96]...</td>\n",
       "      <td>[[tensor(68.3993), tensor(32.4703), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>above</td>\n",
       "      <td>00433</td>\n",
       "      <td>[../../dataset/extractFrame/above/00433/0.jpg,...</td>\n",
       "      <td>[[[[255 255 255], [255 255 255], [255 255 255]...</td>\n",
       "      <td>[[tensor(74.7580), tensor(29.8861), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>above</td>\n",
       "      <td>00435</td>\n",
       "      <td>[../../dataset/extractFrame/above/00435/0.jpg,...</td>\n",
       "      <td>[[[[43 43 42], [43 43 42], [44 44 43], [44 44 ...</td>\n",
       "      <td>[[tensor(93.5003), tensor(29.0611), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>above</td>\n",
       "      <td>65004</td>\n",
       "      <td>[../../dataset/extractFrame/above/65004/0.jpg,...</td>\n",
       "      <td>[[[[ 92 102  26], [ 95 105  29], [ 95 105  29]...</td>\n",
       "      <td>[[tensor(81.5529), tensor(21.3751), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>yourself</td>\n",
       "      <td>64445</td>\n",
       "      <td>[../../dataset/extractFrame/yourself/64445/0.j...</td>\n",
       "      <td>[[[[ 61 127 207], [ 67 136 215], [ 69 141 219]...</td>\n",
       "      <td>[[tensor(60.0000), tensor(33.7012), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>yourself</td>\n",
       "      <td>64446</td>\n",
       "      <td>[../../dataset/extractFrame/yourself/64446/0.j...</td>\n",
       "      <td>[[[[0 4 3], [0 3 1], [0 3 1], [1 4 2], [0 5 4]...</td>\n",
       "      <td>[[tensor(99.8190), tensor(27.4796), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>yourself</td>\n",
       "      <td>64447</td>\n",
       "      <td>[../../dataset/extractFrame/yourself/64447/0.j...</td>\n",
       "      <td>[[[[3 9 8], [3 5 6], [5 7 8], [ 6  7 11], [2 8...</td>\n",
       "      <td>[[tensor(97.8209), tensor(26.3427), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>yourself</td>\n",
       "      <td>64448</td>\n",
       "      <td>[../../dataset/extractFrame/yourself/64448/0.j...</td>\n",
       "      <td>[[[[135 155 126], [134 154 125], [139 159 130]...</td>\n",
       "      <td>[[tensor(65.0531), tensor(29.4849), tensor(0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>yourself</td>\n",
       "      <td>64451</td>\n",
       "      <td>[../../dataset/extractFrame/yourself/64451/0.j...</td>\n",
       "      <td>[[[[255 255 255], [255 255 255], [255 255 255]...</td>\n",
       "      <td>[[tensor(77.4464), tensor(26.6553), tensor(0.9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class folder                                               file  \\\n",
       "0       above  00430  [../../dataset/extractFrame/above/00430/0.jpg,...   \n",
       "1       above  00431  [../../dataset/extractFrame/above/00431/0.jpg,...   \n",
       "2       above  00433  [../../dataset/extractFrame/above/00433/0.jpg,...   \n",
       "3       above  00435  [../../dataset/extractFrame/above/00435/0.jpg,...   \n",
       "4       above  65004  [../../dataset/extractFrame/above/65004/0.jpg,...   \n",
       "..        ...    ...                                                ...   \n",
       "546  yourself  64445  [../../dataset/extractFrame/yourself/64445/0.j...   \n",
       "547  yourself  64446  [../../dataset/extractFrame/yourself/64446/0.j...   \n",
       "548  yourself  64447  [../../dataset/extractFrame/yourself/64447/0.j...   \n",
       "549  yourself  64448  [../../dataset/extractFrame/yourself/64448/0.j...   \n",
       "550  yourself  64451  [../../dataset/extractFrame/yourself/64451/0.j...   \n",
       "\n",
       "                                                 image  \\\n",
       "0    [[[[6 6 6], [6 6 6], [6 6 6], [8 8 8], [6 6 6]...   \n",
       "1    [[[[128 161  99], [126 159  98], [124 157  96]...   \n",
       "2    [[[[255 255 255], [255 255 255], [255 255 255]...   \n",
       "3    [[[[43 43 42], [43 43 42], [44 44 43], [44 44 ...   \n",
       "4    [[[[ 92 102  26], [ 95 105  29], [ 95 105  29]...   \n",
       "..                                                 ...   \n",
       "546  [[[[ 61 127 207], [ 67 136 215], [ 69 141 219]...   \n",
       "547  [[[[0 4 3], [0 3 1], [0 3 1], [1 4 2], [0 5 4]...   \n",
       "548  [[[[3 9 8], [3 5 6], [5 7 8], [ 6  7 11], [2 8...   \n",
       "549  [[[[135 155 126], [134 154 125], [139 159 130]...   \n",
       "550  [[[[255 255 255], [255 255 255], [255 255 255]...   \n",
       "\n",
       "                                                YOLO11  \n",
       "0    [[tensor(88.6293), tensor(19.6030), tensor(0.9...  \n",
       "1    [[tensor(68.3993), tensor(32.4703), tensor(0.9...  \n",
       "2    [[tensor(74.7580), tensor(29.8861), tensor(0.9...  \n",
       "3    [[tensor(93.5003), tensor(29.0611), tensor(0.9...  \n",
       "4    [[tensor(81.5529), tensor(21.3751), tensor(0.9...  \n",
       "..                                                 ...  \n",
       "546  [[tensor(60.0000), tensor(33.7012), tensor(0.9...  \n",
       "547  [[tensor(99.8190), tensor(27.4796), tensor(0.9...  \n",
       "548  [[tensor(97.8209), tensor(26.3427), tensor(0.9...  \n",
       "549  [[tensor(65.0531), tensor(29.4849), tensor(0.9...  \n",
       "550  [[tensor(77.4464), tensor(26.6553), tensor(0.9...  \n",
       "\n",
       "[551 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeImage.to_pickle(\"../../dataset/pickle_YOLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
