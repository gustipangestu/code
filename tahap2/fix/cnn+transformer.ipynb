{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class VideoRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_frames, embed_dim, num_heads, num_layers, hidden_dim):\n",
    "        super(VideoRecognitionModel, self).__init__()\n",
    "        \n",
    "        # Load a pre-trained MobileNet model from timm\n",
    "        self.mobilenet = timm.create_model('mobilenetv3_large_100', pretrained=True, features_only=True)\n",
    "        \n",
    "        # Remove the last layer to get feature maps\n",
    "        self.mobilenet.global_pool = nn.Identity()\n",
    "        self.mobilenet.classifier = nn.Identity()\n",
    "        \n",
    "        # Feature dimension from MobileNet\n",
    "        self.feature_dim = 960  # For mobilenetv3_large_100, adjust if using a different model\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
    "        encoder_layers = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, channels, height, width)\n",
    "        batch_size, num_frames, channels, height, width = x.shape\n",
    "        \n",
    "        # Process each frame through MobileNet\n",
    "        features = []\n",
    "        for t in range(num_frames):\n",
    "            frame = x[:, t, :, :, :]  # Extract frame at time t\n",
    "            frame_features = self.mobilenet(frame)  # Extract features using MobileNet\n",
    "            frame_features = frame_features[-1]  # Use the last feature map\n",
    "            frame_features = frame_features.mean([2, 3])  # Global average pooling\n",
    "            features.append(frame_features)\n",
    "        \n",
    "        # Stack features along the time dimension\n",
    "        features = torch.stack(features, dim=1)  # Shape: (batch_size, num_frames, feature_dim)\n",
    "        \n",
    "        # Project features to the embedding dimension\n",
    "        features = nn.Linear(self.feature_dim, self.embed_dim)(features)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        features = features + self.positional_encoding\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(features)  # Shape: (batch_size, num_frames, embed_dim)\n",
    "        \n",
    "        # Aggregate over time (e.g., mean pooling)\n",
    "        aggregated_output = transformer_output.mean(dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(aggregated_output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d4db757f094867adbd878e4e583d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\intel\\.cache\\huggingface\\hub\\models--timm--mobilenetv3_large_100.ra_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "d:\\miniconda\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "    # Hyperparameters\n",
    "num_classes = 10  # Number of classes for classification\n",
    "num_frames = 16   # Number of frames in the video\n",
    "embed_dim = 512   # Embedding dimension for Transformer\n",
    "num_heads = 8     # Number of attention heads\n",
    "num_layers = 2    # Number of Transformer layers\n",
    "hidden_dim = 1024 # Hidden dimension in Transformer feed-forward network\n",
    "    \n",
    "    # Initialize model\n",
    "model = VideoRecognitionModel(num_classes, num_frames, embed_dim, num_heads, num_layers, hidden_dim)\n",
    "    \n",
    "    # Dummy input (batch_size, num_frames, channels, height, width)\n",
    "dummy_input = torch.randn(2, num_frames, 3, 224, 224)\n",
    "    \n",
    "    # Forward pass\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)  # Should be (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0126, -0.0574, -0.0610, -0.1870,  0.2715, -0.2460, -0.0294, -0.2435,\n",
      "          0.0934, -0.2143],\n",
      "        [ 0.2715, -0.0578, -0.1424, -0.3074,  0.2688, -0.2412,  0.1157, -0.2850,\n",
      "          0.1176, -0.1314]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
