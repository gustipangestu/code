{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.fft import fft, ifft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image\n",
    "\n",
    "\n",
    "\n",
    "def getFiturLandmark(img):\n",
    "    err = None\n",
    "    detection_result = []\n",
    "    try:\n",
    "        \n",
    "        base_options = python.BaseOptions(model_asset_path='pose_landmarker_heavy.task')\n",
    "        options = vision.PoseLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        output_segmentation_masks=True)\n",
    "        detector = vision.PoseLandmarker.create_from_options(options)\n",
    "        \n",
    "        detection_result = detector.detect(img)\n",
    "\n",
    "        err = None\n",
    "    \n",
    "    except:\n",
    "        err = 1\n",
    "        detection_result = []\n",
    "    \n",
    "    return detection_result, err\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # concate \n",
    "def flatten(data):\n",
    "    a = np.ravel(data)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "     transforms.RandomRotation(degrees=(30, 70)),\n",
    "    transforms.Resize((100, 100)),\n",
    "    # transforms.Grayscale(1),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "   \n",
    "    transforms.ToTensor(),\n",
    "   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'above', 1: 'accomplish', 2: 'adopt', 3: 'advantage', 4: 'alphabet', 5: 'anniversary', 6: 'another', 7: 'appropriate', 8: 'arrogant', 9: 'artist', 10: 'background', 11: 'bee', 12: 'behavior', 13: 'bell', 14: 'birth', 15: 'bless', 16: 'blood', 17: 'boots', 18: 'brave', 19: 'breathe', 20: 'bright', 21: 'bull', 22: 'butterfly', 23: 'card', 24: 'category', 25: 'catholic', 26: 'cent', 27: 'chemistry', 28: 'cherry', 29: 'china', 30: 'chop', 31: 'christian', 32: 'cigarette', 33: 'clever', 34: 'clown', 35: 'coach', 36: 'coat', 37: 'cochlear implant', 38: 'coconut', 39: 'common', 40: 'commute', 41: 'control', 42: 'count', 43: 'culture', 44: 'daily', 45: 'defend', 46: 'degree', 47: 'demonstrate', 48: 'department', 49: 'die', 50: 'dig', 51: 'dinner', 52: 'dinosaur', 53: 'diploma', 54: 'director', 55: 'disconnect', 56: 'discover', 57: 'don_t want', 58: 'drum', 59: 'pervideo', 60: 'sun', 61: 'support', 62: 'suppose', 63: 'sure', 64: 'surprise', 65: 'sweater', 66: 'swim', 67: 'symbol', 68: 'taste', 69: 'team', 70: 'telephone', 71: 'tennis', 72: 'their', 73: 'then', 74: 'thermometer', 75: 'thing', 76: 'third', 77: 'thousand', 78: 'three', 79: 'tie', 80: 'topic', 81: 'touch', 82: 'tournament', 83: 'try', 84: 'two', 85: 'type', 86: 'umbrella', 87: 'vegetable', 88: 'vocabulary', 89: 'waste', 90: 'we', 91: 'wear', 92: 'weekly', 93: 'welcome', 94: 'winter', 95: 'without', 96: 'wolf', 97: 'worm', 98: 'wristwatch', 99: 'yourself'}\n",
      "above\n",
      "above\n",
      "accomplish\n",
      "adopt\n",
      "advantage\n",
      "alphabet\n",
      "anniversary\n",
      "another\n",
      "appropriate\n",
      "arrogant\n",
      "artist\n",
      "background\n",
      "bee\n",
      "behavior\n",
      "bell\n",
      "birth\n",
      "bless\n",
      "blood\n",
      "boots\n",
      "brave\n",
      "breathe\n",
      "bright\n",
      "bull\n",
      "butterfly\n",
      "card\n",
      "category\n",
      "catholic\n",
      "cent\n",
      "chemistry\n",
      "cherry\n",
      "china\n",
      "chop\n",
      "christian\n",
      "cigarette\n",
      "clever\n",
      "clown\n",
      "coach\n",
      "coat\n",
      "cochlear implant\n",
      "coconut\n",
      "common\n",
      "commute\n",
      "control\n",
      "count\n",
      "culture\n",
      "daily\n",
      "defend\n",
      "degree\n",
      "demonstrate\n",
      "department\n",
      "die\n",
      "dig\n",
      "dinner\n",
      "dinosaur\n",
      "diploma\n",
      "director\n",
      "disconnect\n",
      "discover\n",
      "don_t want\n",
      "drum\n",
      "pervideo\n",
      "sun\n",
      "support\n",
      "suppose\n",
      "sure\n",
      "surprise\n",
      "sweater\n",
      "swim\n",
      "symbol\n",
      "taste\n",
      "team\n",
      "telephone\n",
      "tennis\n",
      "their\n",
      "then\n",
      "thermometer\n",
      "thing\n",
      "third\n",
      "thousand\n",
      "three\n",
      "tie\n",
      "topic\n",
      "touch\n",
      "tournament\n",
      "try\n",
      "two\n",
      "type\n",
      "umbrella\n",
      "vegetable\n",
      "vocabulary\n",
      "waste\n",
      "we\n",
      "wear\n",
      "weekly\n",
      "welcome\n",
      "winter\n",
      "without\n",
      "wolf\n",
      "worm\n",
      "wristwatch\n",
      "yourself\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dictClass = {}\n",
    "i = 0\n",
    "for _i in os.listdir('../../dataset/extractFrame'):\n",
    "    # print(_i)\n",
    "    dictClass[i] = _i\n",
    "    i = i + 1\n",
    "\n",
    "print(dictClass)\n",
    "print(dictClass[0])\n",
    "for i in dictClass:\n",
    "    print(dictClass[i])\n",
    "    \n",
    "_idClass = [_key for _key in dictClass if dictClass[_key] == \"wolf\"]\n",
    "print(int(_idClass[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset class\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "\n",
    "def toTensor(_x):\n",
    "    # _em = torch.rand(168,51)\n",
    "    # _newX = torch.as_tensor(_x)\n",
    "    # _padSeq = pad_sequence([_em, _newX], batch_first=True, padding_value=0)\n",
    "    # _xDt = pd.Series(_x)\n",
    "    return _x\n",
    "\n",
    "\n",
    "\n",
    "class dataClass(Dataset):\n",
    "    def __init__(self, pickle_data, root_dir, transform=None):\n",
    "        self.file = pd.read_pickle(pickle_data)\n",
    "        self.input = torch.tensor\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # self.dataFitur = [np.array(_x) ]\n",
    "        # self.dataScaled = standardScaler()\n",
    "        \n",
    "        self.yoloTensor = [torch.tensor(np.array(standardScaler.fit_transform(_x))) for _x in self.file['YOLO11']]\n",
    "        self.yoloTensorPad = pad_sequence(self.yoloTensor, batch_first=True)\n",
    "        \n",
    "        # self.yoloTensorPackPad = pack_padded_sequence(self.yoloTensorPad, lengths=self.leng, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.yoloTensorPad)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        _class = self.file.iloc[index]['class']\n",
    "        _image = self.file.iloc[index]['image']\n",
    "        _fitur = self.yoloTensorPad[index].float()\n",
    "        _idClass = [_key for _key in dictClass if dictClass[_key] == _class]\n",
    "        \n",
    "        _datareturn = {\n",
    "                    'dataPack': _fitur,\n",
    "                    'target': _class,\n",
    "                    'idClass': [_idClass[0]],\n",
    "                    # 'image': _image\n",
    "                       }\n",
    "        \n",
    "            \n",
    "        return _datareturn\n",
    "\n",
    "data_torch = dataClass(pickle_data=\"../../dataset/pickle_YOLO\", root_dir=\"../../dataset/extractFrame\", transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = pd.read_pickle(\"../../dataset/pickle_YOLO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled = standardScaler.fit_transform(aaa['YOLO11'][0])\n",
    "# print(scaled)\n",
    "# print(aaa['YOLO11'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6293,  0.6821, -0.7355,  ...,  0.0000,  0.0000, -0.2833],\n",
      "        [ 2.2485,  0.5858, -0.7390,  ...,  0.0000,  0.0000, -0.2673],\n",
      "        [ 2.1693,  0.5470, -0.6406,  ...,  0.0000,  0.0000, -0.2821],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "boots\n",
      "[17]\n"
     ]
    }
   ],
   "source": [
    "print(data_torch[90]['dataPack'])\n",
    "print(data_torch[90]['target'])\n",
    "print(data_torch[90]['idClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# split dataset\n",
    "print(len(data_torch))\n",
    "train_data, val_data = random_split(data_torch, [0.80, 0.20])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch #pytorch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True, bidirectional=False) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        hn = 0\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, self.hidden_size)) #internal state\n",
    "        _leng = [len(_i) for _i in x]\n",
    "        _packpad = pack_padded_sequence(x, lengths=_leng, batch_first=True, enforce_sorted=False)\n",
    "        # print(x.shape)\n",
    "        # print(_packpad)\n",
    "        \n",
    "        output, (hn, cn) = self.lstm(_packpad) #lstm with input, hidden, and internal state\n",
    "        \n",
    "        \n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer nhidd\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        # out = self.softmax(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        # out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# train\n",
    "num_epochs = 1000 #1000 epochs\n",
    "learning_rate = 0.1 #0.001 lr\n",
    "\n",
    "input_size = 51 #number of features\n",
    "hidden_size = 20 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 100 #number of output classes \n",
    "\n",
    "\n",
    "lstm1 = LSTM1(num_classes=num_classes, hidden_size=hidden_size,\n",
    "              input_size=input_size, num_layers=num_layers, seq_length=128) #our lstm class sss\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm1.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _i, _v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     10\u001b[0m         total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m         total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32md:\\miniconda\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[6], line 45\u001b[0m, in \u001b[0;36mdataClass.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     43\u001b[0m _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39miloc[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m _image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39miloc[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 45\u001b[0m _fitur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myoloTensorPad[index]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     46\u001b[0m _idClass \u001b[38;5;241m=\u001b[39m [_key \u001b[38;5;28;01mfor\u001b[39;00m _key \u001b[38;5;129;01min\u001b[39;00m dictClass \u001b[38;5;28;01mif\u001b[39;00m dictClass[_key] \u001b[38;5;241m==\u001b[39m _class]\n\u001b[0;32m     48\u001b[0m _datareturn \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataPack\u001b[39m\u001b[38;5;124m'\u001b[39m: _fitur,\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: _class,\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midClass\u001b[39m\u001b[38;5;124m'\u001b[39m: [_idClass[\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;66;03m# 'image': _image\u001b[39;00m\n\u001b[0;32m     53\u001b[0m                }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "# _leng = [len(_i) for _i in x]\n",
    "# _packpad = pack_padded_sequence(x, lengths=_leng, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    for _i, _v in enumerate(train_dataloader):\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        batch_loss = 0\n",
    "        # print(_v['dataPack'].shape)\n",
    "        for _in in range (0, train_dataloader.batch_size-1):\n",
    "            \n",
    "            # print(_v['dataPack'][_in])\n",
    "        \n",
    "           \n",
    "            _fitur = _v['dataPack'][_in]\n",
    "            # rubah dimensi menjadi (layer, seq length, fitur)\n",
    "            _fitur = torch.permute(_fitur, (1,0,2)).size()\n",
    "            _class = torch.tensor(_v['idClass'][_in])\n",
    "            print(_class)\n",
    "            # print(_class.shape)\n",
    "            outputs = lstm1(_fitur)\n",
    "            # print(outputs)\n",
    "            \n",
    "            loss = criterion(outputs, _class)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == _class).sum().item()\n",
    "        \n",
    "            total_samples += _class\n",
    "            \n",
    "            batch_loss += loss / len(_v)\n",
    "            correct = (predicted == _class).sum().item() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "    # current_loss += batch_loss.item() / len(_v)\n",
    "    # validation_accuracy = total_correct / total_samples\n",
    "\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {batch_loss}')\n",
    "    # print(f'Validation Accuracy: {validation_accuracy:.4f}')\n",
    "    # all_losses.append(current_loss / len(_batch) )\n",
    "    # current_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
